# app.py
import streamlit as st
import re
import json
import pdfplumber
import pandas as pd
import dateparser
import phonenumbers
from email_validator import validate_email, EmailNotValidError
import spacy
from rapidfuzz import fuzz
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from io import StringIO
from datetime import datetime
import nltk
from nltk.corpus import wordnet
from functools import lru_cache
import tempfile, os
import unicodedata  # â† Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
import numpy as np
import plotly.graph_objects as go
from sentence_transformers import SentenceTransformer, util

def normalize_text(txt: str) -> str:
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÛŒÙˆÙ†ÛŒÚ©Ø¯ ØªØ§ Ø­Ø±ÙˆÙ Ù„Ù‡Ø¬Ù‡â€ŒØ¯Ø§Ø± Ø¯Ø±Ø³Øª Ø¨Ø§Ø´Ù†Ø¯ (JoÃ£o Ùˆ â€¦)
    return unicodedata.normalize("NFC", txt)

def _keep_letters_spaces(s: str) -> str:
    # ÙÙ‚Ø· Ù†ÙˆÛŒØ³Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø±ÙÛŒ Ù‡Ù…Ù‡Ù” Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ + ÙØ§ØµÙ„Ù‡/Ø¢Ù¾Ø§Ø³ØªØ±ÙˆÙ/Ø®Ø·â€ŒØªÛŒØ±Ù‡ Ø±Ø§ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±Ø¯
    return "".join(ch for ch in s if unicodedata.category(ch).startswith("L") or ch in " '-")


# ---------- Cache-heavy resources ----------
@st.cache_resource(show_spinner=False)
def get_spacy():
    try:
        return spacy.load("en_core_web_sm")
    except Exception as e:
        st.warning(f"spaCy model load failed: {e}")
        return None

# ---- Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†Ù ØªØ§Ø¨Ø¹ get_st_model Ùˆ Ù†Ù…Ø§ÛŒØ´ ÙˆØ¶Ø¹ÛŒØª ----
@st.cache_resource(show_spinner=True)
def get_st_model():
    """
    ØªÙ„Ø§Ø´ Ú†Ù†Ø¯Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ SBERT:
    1) Ù…Ø³ÛŒØ± Ù…Ø­Ù„ÛŒ ./models/all-MiniLM-L6-v2 (Ø§Ú¯Ø± Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒ)
    2) Ù†Ø§Ù… Ø§ØµÙ„ÛŒ "all-MiniLM-L6-v2"
    3) Ù…Ø¯Ù„ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø³Ø¨Ú©â€ŒØªØ± "paraphrase-MiniLM-L6-v2"
    Ø§Ú¯Ø± Ù‡Ù…Ù‡ Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯ØŒ None Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    try_paths = [
        "./models/all-MiniLM-L6-v2",   # Ù…Ø³ÛŒØ± Ù…Ø­Ù„ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        "all-MiniLM-L6-v2",
        "sentence-transformers/paraphrase-MiniLM-L6-v2",
    ]
    last_err = None
    for name in try_paths:
        try:
            m = SentenceTransformer(name)
            # ØªØ³Øª Ø®ÛŒÙ„ÛŒ Ø³Ø¨Ú©: Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ø¯Ùˆ Ø¬Ù…Ù„Ù‡
            _ = m.encode(["test a", "test b"], convert_to_tensor=True, normalize_embeddings=True)
            return m
        except Exception as e:
            last_err = e
    # Ø§Ú¯Ø± Ù†Ø±Ø³ÛŒØ¯ÛŒÙ…
    st.sidebar.error(f"SBERT failed to load. Last error: {last_err}")
    return None

st_model = get_st_model()


@st.cache_resource(show_spinner=False)
def ensure_wordnet():
    try:
        _ = wordnet.synsets("test")
    except LookupError:
        try:
            nltk.download('wordnet')
        except Exception as e:
            st.warning(f"Could not download WordNet: {e}")

nlp = get_spacy()
ensure_wordnet()

# ---------- Helpers ----------
def semantic_similarity(text1, text2, threshold=0.55):
    if st_model is None:
        return False, 0.0
    emb1 = st_model.encode(text1, convert_to_tensor=True, normalize_embeddings=True)
    emb2 = st_model.encode(text2, convert_to_tensor=True, normalize_embeddings=True)
    sim = float(util.pytorch_cos_sim(emb1, emb2).item())
    return sim >= threshold, sim

EMAIL_REGEX = re.compile(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b")
FALLBACK_PHONE_REGEX = re.compile(
    r"(?:\+?\d{1,3}[\s\-.]?)?"
    r"(?:\(?\d{3}\)?[\s\-.]?)"
    r"\d{3}[\s\-.]?\d{4}"
)

GPA_PATTERN = r"([0-9]+(?:\.[0-9]{1,2})?)\s*/\s*([0-9]+(?:\.[0-9]{1,2})?)"

DEGREES = [
    ("PhD",      ["phd", "ph.d", "doctor", "doctoral"], 5),
    ("Master",   ["master", "m.sc", "m.s", "ms"],       4),
    ("Bachelor", ["bachelor", "b.sc", "b.s", "ba"],     3),
    ("Associate",["associate", "a.sc", "a.s"],          2),
    ("High School", ["high school", "diploma"],         1),
]

FUZZY_THRESHOLD_SKILLS = 80

# ---------- WordNet expansion with caching ----------
@lru_cache(maxsize=2048)
def _expand_one_term(t: str):
    ex = {t.lower()}
    try:
        for syn in wordnet.synsets(t):
            for lemma in syn.lemmas():
                ex.add(lemma.name().replace('_', ' ').lower())
    except Exception:
        pass
    return ex

def expand_keywords(keywords: list) -> list:
    expanded = set()
    for kw in keywords:
        expanded |= _expand_one_term(kw)
    return list(expanded)

def expand_terms(terms):
    expanded = set()
    for t in terms:
        expanded |= _expand_one_term(t)
    return expanded

SECTION_KEYWORDS = {
    "education": [
        "education", "education & training", "academic background",
        "academic qualifications", "academic history", "training",
        "certifications", "courses"
    ],
    "experience": [
        "experience", "work experience", "professional experience",
        "employment", "employment history", "work history",
        "career history", "career summary"
    ],
    "skills": [
        "skills", "technical skills", "tech skills", "skills & technologies",
        "technologies", "core competencies", "competencies",
        "areas of expertise", "toolbox", "tooling"
    ]
}

_BULLET = r"(?:(?:[\-\*\u2022]\s*)?)"      # -, *, â€¢
_ENDPUNCT = r"(?:\s*[:\-â€“â€”]?)*"            # :, -, â€“, â€”
_MD_PREFIX = r"(?:#{1,6}\s*)?"             # markdown '## '

def _norm_line(s: str) -> str:
    s = normalize_text(s)
    s = s.replace("\t", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _looks_like_header_text(s: str) -> bool:
    if not s: return False
    if len(s) > 60: return False
    if "@" in s or "http" in s.lower(): return False
    if s.endswith("."): return False
    if len(re.findall(r"\d", s)) > 4: return False
    return True

HDR_PATTERNS = {}
for sec, keys in SECTION_KEYWORDS.items():
    variants = []
    for k in keys:
        k_esc = re.escape(k)
        # Ø¯Ù‚ÛŒÙ‚Ø§ ØªÙˆØ§Ù„ÛŒ Â«\&Â» Ùˆ Â«\-Â» Ø±Ø§ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (Ø¯ÛŒÚ¯Ù‡ re.sub Ù„Ø§Ø²Ù… Ù†ÛŒØ³Øª)
        k_esc = k_esc.replace(r'\&', r'(?:\s*&\s*|\s+and\s+)')
        k_esc = k_esc.replace(r'\-', r'(?:\-|\s+)')
        variants.append(k_esc)
    pat = rf"^{_MD_PREFIX}{_BULLET}(?:{'|'.join(variants)}){_ENDPUNCT}$"
    HDR_PATTERNS[sec] = re.compile(pat, flags=re.IGNORECASE)



def _is_underline(line: str) -> bool:
    return bool(re.fullmatch(r"\s*[\-â€“â€”_=]{3,}\s*", line))

def _detect_header(line: str) -> str | None:
    L = _norm_line(line)
    if not _looks_like_header_text(L):
        return None
    for sec, pat in HDR_PATTERNS.items():
        if pat.search(L):
            return sec
    low = L.lower()
    if re.search(r"\bexperience with\b", low):
        return None
    for sec, keys in SECTION_KEYWORDS.items():
        for k in keys:
            if low.startswith(k.lower()) and len(L) <= 70:
                return sec
    return None

def split_into_sections(raw: str) -> dict:
    sections = {k: [] for k in SECTION_KEYWORDS}
    sections["other"] = []
    lines = raw.split("\n")
    current = "other"
    i = 0
    while i < len(lines):
        line = lines[i]
        normed = _norm_line(line)
        # underline style
        if _looks_like_header_text(normed) and (i + 1) < len(lines) and _is_underline(lines[i+1]):
            sec = _detect_header(normed) or _detect_header(normed.replace(":", ""))
            if sec:
                current = sec
                i += 2
                continue
        # single-line header
        sec = _detect_header(line)
        if sec:
            current = sec
            i += 1
            continue
        if normed:
            sections[current].append(line.rstrip())
        i += 1
    return {k: "\n".join(v) for k, v in sections.items()}


def extract_text_from_pdf(uploaded_file):
    # 1) pdfplumber
    try:
        uploaded_file.seek(0)
        with pdfplumber.open(uploaded_file) as pdf:
            txt = '\n'.join(p.extract_text() or '' for p in pdf.pages)
        if len(txt.strip()) > 100:
            return txt
    except Exception as e:
        st.warning(f"pdfplumber extraction failed: {str(e)}")
    # 2) pdfminer
    try:
        uploaded_file.seek(0)
        mgr = PDFResourceManager()
        ret = StringIO()
        dev = TextConverter(mgr, ret, laparams=LAParams())
        ip = PDFPageInterpreter(mgr, dev)
        for pg in PDFPage.get_pages(uploaded_file):
            ip.process_page(pg)
        dev.close()
        out = ret.getvalue()
        ret.close()
        if len(out.strip()) > 100:
            return out
    except Exception as e:
        st.warning(f"PDFMiner extraction failed: {str(e)}")
    # 3) OCR (needs poppler & tesseract on system)
    try:
        uploaded_file.seek(0)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(uploaded_file.read())
            tmp_path = tmp.name
        try:
            from pdf2image import convert_from_path
            import pytesseract
            images = convert_from_path(tmp_path, dpi=300)
            text_ocr = '\n'.join(pytesseract.image_to_string(img, lang='eng') for img in images)
            return text_ocr
        finally:
            os.unlink(tmp_path)
    except Exception as e:
        st.error("OCR failed (Poppler & Tesseract required). Details: " + str(e))
        return ''

def extract_email(raw: str) -> str:
    for m in EMAIL_REGEX.finditer(raw):
        cand = m.group(0).strip('.,;:')
        try:
            return validate_email(cand).email
        except EmailNotValidError:
            continue
        except Exception:
            continue
    return 'N/A'

def extract_phone(raw: str, default_region='US') -> str:
    text = raw.replace('\u00A0',' ')
    text = text.translate(str.maketrans({
        '\u2018': "'", '\u2019': "'", '\u201C':'"', '\u201D':'"'
    }))
    text = re.sub(r'[ \t]+',' ', text)
    for m in phonenumbers.PhoneNumberMatcher(text, default_region):
        return phonenumbers.format_number(m.number, phonenumbers.PhoneNumberFormat.INTERNATIONAL)
    mo = FALLBACK_PHONE_REGEX.search(text)
    return mo.group(0).strip() if mo else 'N/A'

# Name affixes cleanup
def strip_name_affixes(name_str: str) -> str:
    name_str = normalize_text(name_str)  # â† Ø§ÛŒÙ† Ø®Ø· Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
    PREFIXES = {"dr", "mr", "ms", "mrs", "prof"}
    SUFFIXES = {"phd", "msc", "bsc"}
    tokens = [t for t in re.split(r"\s+", name_str.strip()) if t]
    def norm(t: str) -> str:
        return re.sub(r"[^A-Za-z]", "", t).lower()
    while tokens and norm(tokens[0]) in PREFIXES:
        tokens.pop(0)
    while tokens and norm(tokens[-1].rstrip(",.;")) in SUFFIXES:
        tokens.pop()
    tokens = [t.strip(",.; ") for t in tokens if t.strip(",.; ")]
    return " ".join(tokens)

def extract_name(raw: str) -> tuple:
    text = normalize_text(raw)

    STOPWORDS = {
        "email","e-mail","mail","phone","tel","mobile","cell","contact",
        "location","address","linkedin","github","website","profile"
    }

    def cut_at_stopwords(parts: list) -> list:
        """parts Ø±Ø§ ØªØ§ Ù‚Ø¨Ù„ Ø§Ø² Ø§ÙˆÙ„ÛŒÙ† Ú©Ù„Ù…Ù‡Ù” Ù…Ø²Ø§Ø­Ù… Ú©ÙˆØªØ§Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        cut = len(parts)
        for i, tok in enumerate(parts):
            if tok.lower() in STOPWORDS:
                cut = i
                break
        return parts[:cut]

    # 1) ØªÙ„Ø§Ø´ Ø¨Ø§ NER Ø§Ø³Ù¾Ø§Ø³ÛŒ
    if nlp:
        try:
            doc = nlp(text)
            for ent in doc.ents:
                if ent.label_ == "PERSON":
                    cleaned = strip_name_affixes(ent.text)
                    parts = [p for p in _keep_letters_spaces(cleaned).split() if p]
                    parts = cut_at_stopwords(parts)  # â† Ø¨Ø±ÛŒØ¯Ù† Ø­ØªÛŒ Ø¯Ø± Ù…Ø³ÛŒØ± NER
                    if len(parts) >= 2:
                        first = parts[0].title()
                        last  = " ".join(parts[1:]).title()
                        return first, last
        except Exception:
            pass

    # 2) ÙØ§Ù„â€ŒØ¨Ú©: Ø®Ø·â€ŒØ¨Ù‡â€ŒØ®Ø·
    for ln in text.split("\n"):
        ln = ln.strip()
        if not ln:
            continue

        # Ø§Ú¯Ø± ":" ÛŒØ§ "|" Ù‡Ø³ØªØŒ ÙÙ‚Ø· Ø¨Ø®Ø´ Ù‚Ø¨Ù„ Ø§Ø² Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ú¯ÛŒØ±
        head = ln.split("|")[0]
        head = head.split(":")[0]

        cleaned = strip_name_affixes(head)
        parts = [p for p in _keep_letters_spaces(cleaned).split() if p]
        if len(parts) < 2:
            # Ø§Ú¯Ø± Ø¨Ø¹Ø¯ Ø§Ø² Ø¨Ø±ÛŒØ¯Ù†Ù Ø³Ø± Ø®Ø· Ù‡Ù†ÙˆØ² Ø¯Ùˆ Ú©Ù„Ù…Ù‡ Ù†Ø´Ø¯ØŒ Ú©Ù„ ln Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†
            cleaned = strip_name_affixes(ln)
            parts = [p for p in _keep_letters_spaces(cleaned).split() if p]
        parts = cut_at_stopwords(parts)

        if len(parts) >= 2:
            first = parts[0].title()
            last  = " ".join(parts[1:]).title()
            return first, last

    return "N/A", "N/A"

# --------- Degree keyword cache ---------
DEG_KEYS = {}
for label, syns, lvl in DEGREES:
    base = {label.lower(), *syns}
    ex = set()
    for term in base:
        ex |= _expand_one_term(term)
    DEG_KEYS[label] = ex
    
# ===== Auto synonym discovery (generic for ANY skills) =====
ALNUM_DASH = re.compile(r"[^a-z0-9\-\+\.# ]+")
STOP_TOKENS = {"and","or","of","the","a","an","in","on","for","with","to","by","at","from"}

def _norm(s: str) -> str:
    s = normalize_text(s).lower()
    s = s.replace("&"," and ").replace("/", " ").replace("-", " ")
    return re.sub(r"\s+", " ", s).strip()

def _tokenize_words(s: str): 
    s = _norm(s)
    return [w for w in ALNUM_DASH.sub(" ", s).split() if w]

def _phrase_ok(ph: str) -> bool:
    if len(ph) < 3 or ph in STOP_TOKENS: return False
    if ph.isdigit() or ph.count(" ") > 3: return False
    return True


def _section_texts(txt: str) -> str:
    secs = split_into_sections(txt)
    return " \n ".join([
        secs.get('skills',''),
        secs.get('experience',''),
        secs.get('education',''),  # â† Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
        secs.get('other','')
    ])

def collect_corpus_candidates(resume_files, max_docs=200, max_vocab=5000):
    freq = {}
    docs = 0
    for f in resume_files:
        try:
            raw = extract_text_from_pdf(f)
            if not raw or len(raw) < 50: continue
            bag = _section_texts(normalize_text(raw).lower())
            words = _tokenize_words(bag)
            # 1-gram ØªØ§ 3-gram
            L = len(words)
            seen = set()
            for n in (1,2,3):
                for i in range(L-n+1):
                    cand = " ".join(words[i:i+n])
                    if _phrase_ok(cand) and cand not in seen:
                        seen.add(cand)
                        freq[cand] = freq.get(cand, 0) + 1
            docs += 1
            if docs >= max_docs: break
        except Exception:
            continue
    return [p for p,_ in sorted(freq.items(), key=lambda kv: kv[1], reverse=True)[:max_vocab]]

def build_skill_synonyms_auto(
    req_skills: list[str],
    corpus_candidates: list[str],
    model,
    topk: int = 12,
    sim_threshold: float = 0.70,
    batch_size: int = 256
):
    """
    Ù‡Ø± Ú©Ø§Ù†Ø¯ÛŒØ¯ (phrase) Ø¨Ù‡ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ù…Ù‡Ø§Ø±ØªÙ canon Ù†Ø³Ø¨Øª Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
    - aliasÙ‡Ø§ÛŒ ÙÙ†ÛŒ Ù…ØªØ¯Ø§ÙˆÙ„ ØªØ²Ø±ÛŒÙ‚ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ (C++, C#, Node.js, sklearn, ...).
    - Ø§Ú¯Ø± model Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª batch Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
    """

    def _batched_encode(items: list[str]):
        if not items:
            return None
        # SentenceTransformer Ø¨Ø§ convert_to_tensor=True â†’ Tensor
        tensors = []
        for i in range(0, len(items), batch_size):
            chunk = items[i:i+batch_size]
            t = model.encode(
                chunk,
                convert_to_tensor=True,
                normalize_embeddings=True
            )
            tensors.append(t)
        import torch  # Ù…Ø·Ù…Ø¦Ù† Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ… Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª Ø­ØªÛŒ Ø§Ú¯Ø± Ø¨Ø§Ù„Ø§ÛŒ ÙØ§ÛŒÙ„ Ø§Ø¶Ø§ÙÙ‡ Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
        return torch.cat(tensors, dim=0) if tensors else None

    canonical_list = [s for s in (req_skills or []) if s and s.strip()]
    if not canonical_list:
        return {}, [], {}

    # --- 1) Ø³Ø§Ø®Øª Ù„ÛŒØ³Øª Ú©Ø§Ù†Ø¯ÛŒØ¯Ù‡Ø§ Ø§Ø² Ú©ÙˆØ±Ù¾ÙˆØ³ + Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙØ±Ù…â€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬
    cand_list = [c for c in (corpus_candidates or []) if _phrase_ok(c)]
    for s in canonical_list:
        s_low = _norm(s)
        cand_list += [s_low, s_low.replace("-", " "), s_low.replace(" ", "-")]
    # ÛŒÚ©ØªØ§
    cand_list = list(dict.fromkeys([_norm(c) for c in cand_list if c]))

    # --- 2) ØªØ²Ø±ÛŒÙ‚ aliasÙ‡Ø§ÛŒ ÙÙ†ÛŒ Ù…ØªØ¯Ø§ÙˆÙ„ (Ù…ÙˆØ±Ø¯ Û¸)
    EXTRA_TECH_ALIASES = {
        "c++": ["c++", "c plus plus"],
        "c#": ["c#", "c sharp"],
        "node.js": ["node.js", "node js", "nodejs"],
        "scikit-learn": ["scikit-learn", "sklearn"],
        "pytorch": ["pytorch", "torch"],
        "tensorflow": ["tensorflow", "tf"],
        "xgboost": ["xgboost", "xg boost"],
        "postgresql": ["postgresql", "postgres", "postgre sql"],
        "mysql": ["mysql", "my sql"],
        "mongodb": ["mongodb", "mongo db"],
        "matplotlib": ["matplotlib", "mpl"],
        "spss": ["spss"],
        "stata": ["stata"],
    }
    for canon in canonical_list:
        for a in EXTRA_TECH_ALIASES.get(_norm(canon), []):
            cand_list.append(_norm(a))

    # ÙØ±Ù…â€ŒÙ‡Ø§ÛŒ ÙØ§ØµÙ„Ù‡/Ø®Ø·â€ŒØªÛŒØ±Ù‡ Ø±Ø§ Ù‡Ù… ØªØ²Ø±ÛŒÙ‚/ÛŒÚ©Ø³Ø§Ù† Ú©Ù†ÛŒÙ…
    for s in list(cand_list):
        cand_list.append(s.replace("-", " "))
        cand_list.append(s.replace(" ", "-"))
    cand_list = list(dict.fromkeys([_norm(c) for c in cand_list if c]))

    # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ù†Ø¯Ø§Ø±ÛŒÙ…: ÙÙ‚Ø· Ù†Ú¯Ø§Ø´ØªÙ Ù‡Ù…Ù†ÙˆØ´ØªÙ‡â€ŒÙ‡Ø§
    if model is None:
        skill_map = {canon: {_norm(canon)} for canon in canonical_list}
        alias2canon = {_norm(canon): canon for canon in canonical_list}
        # Ù†Ú¯Ø§Ø´Øªâ€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ ÙØ±Ù…â€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬ (Ø¨Ø¯ÙˆÙ† Ù…Ø¯Ù„ Ù‡Ù… Ù…ÙÛŒØ¯Ù†)
        for canon in canonical_list:
            key = _norm(canon)
            alias2canon.setdefault(key.replace("-", " "), canon)
            alias2canon.setdefault(key.replace(" ", "-"), canon)
            if key == "c++":
                alias2canon["c plus plus"] = canon
            if key == "c#":
                alias2canon["c sharp"] = canon
            if key == "node.js":
                alias2canon["node js"] = canon
                alias2canon["nodejs"]  = canon
            if key == "scikit-learn":
                alias2canon["sklearn"] = canon
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ skill_map
        for a, c in alias2canon.items():
            skill_map.setdefault(c, set()).add(a)
        return skill_map, canonical_list, alias2canon

    # --- 3) Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§ (Batch)
    skill_embs = _batched_encode(canonical_list)
    cand_embs  = _batched_encode(cand_list)
    sims = util.cos_sim(skill_embs, cand_embs).cpu().numpy()

    # --- 4) Ù†Ø³Ø¨Øªâ€ŒØ¯Ø§Ø¯Ù† Ù‡Ø± Ú©Ø§Ù†Ø¯ÛŒØ¯ Ø¨Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† canon Ø¨Ø§ Ø¢Ø³ØªØ§Ù†Ù‡
    alias2canon: dict[str, str] = {}
    for j, cand in enumerate(cand_list):
        col = sims[:, j]                 # Ø´Ø¨Ø§Ù‡Øª cand Ø¨Ù‡ Ù‡Ù…Ù‡â€ŒÛŒ canonÙ‡Ø§
        i_best = int(np.argmax(col))
        best_sim = float(col[i_best])
        if best_sim >= sim_threshold:
            alias2canon[cand] = canonical_list[i_best]

    # canonÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡ Ø­ØªÙ…Ø§Ù‹ Ù†Ú¯Ø§Ø´Øª Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯
    for canon in canonical_list:
        alias2canon.setdefault(_norm(canon), canon)

    # --- 5) Ù†Ú¯Ø§Ø´Øªâ€ŒÙ‡Ø§ÛŒ Ø®Ø§ØµÙ ÙØ±Ù…â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø§Ø³ØªÙØ§Ø¯Ù‡ (Ù…ÙˆØ±Ø¯ Û¸)
    for canon in canonical_list:
        key = _norm(canon)
        alias2canon.setdefault(key.replace("-", " "), canon)
        alias2canon.setdefault(key.replace(" ", "-"), canon)
        if key == "c++":
            alias2canon["c plus plus"] = canon
        if key == "c#":
            alias2canon["c sharp"] = canon
        if key == "node.js":
            alias2canon["node js"] = canon
            alias2canon["nodejs"]  = canon
        if key == "scikit-learn":
            alias2canon["sklearn"] = canon

    # --- 6) Ø³Ø§Ø®Øª skill_map Ø§Ø² alias2canon
    skill_map: dict[str, set] = {canon: set() for canon in canonical_list}
    for alias, canon in alias2canon.items():
        skill_map[canon].add(alias)

    # --- 7) Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø´ØªÙ† top-k aliasÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø¨Ø§Ù‡Øª
    if topk and topk > 0:
        for idx_canon, canon in enumerate(canonical_list):
            # Ø§Ù†Ø¯ÛŒØ³â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù†Ø¯ÛŒØ¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ Ø§ÛŒÙ† canon Ù†Ø³Ø¨Øª Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯
            idxs = [i for i, cand in enumerate(cand_list) if cand in skill_map[canon]]
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ù¾Ø§ÛŒÙ‡ Ø´Ø¨Ø§Ù‡Øª Ø§ÛŒÙ† canon Ø¨Ù‡ candÙ‡Ø§
            order = sorted(idxs, key=lambda k: sims[idx_canon, k], reverse=True)[:topk]
            kept = {cand_list[k] for k in order}
            # Ø¨Ø±Ø´ skill_map Ùˆ Ù‡Ù…â€ŒØ²Ù…Ø§Ù† Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ alias2canon Ø§Ø¶Ø§ÙÙ‡
            for a in list(skill_map[canon]):
                if a not in kept:
                    skill_map[canon].remove(a)
                    if alias2canon.get(a) == canon:
                        del alias2canon[a]

    return skill_map, canonical_list, alias2canon

def extract_skills(
    raw: str,
    req_skills: list,
    use_semantic: bool = True,
    skill_map: dict | None = None,
    alias2canon: dict | None = None,
    canonical_list: list | None = None
) -> list:
    if not req_skills:
        return ['N/A']

    canonical_list = [s for s in (canonical_list or req_skills) if s and s.strip()]
    if not (skill_map and alias2canon and canonical_list):
        alias2canon = {_norm(s): s for s in canonical_list}
        skill_map   = {s: {_norm(s)} for s in canonical_list}

    text = normalize_text(raw)
    secs = split_into_sections(text)

    # Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ exact/regex
    skills_txt_norm = _norm(secs.get('skills', '') or '')
    exp_txt_norm    = _norm(secs.get('experience', '') or '')
    edu_txt_norm    = _norm(secs.get('education', '') or '')

    # Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… Ø¨Ø±Ø§ÛŒ semantic (Ø¨Ù‡ Ø®Ø·â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ…)
    skills_txt_raw = secs.get('skills', '') or ''
    exp_txt_raw    = secs.get('experience', '') or ''
    edu_txt_raw    = secs.get('education', '') or ''

    found = set()

    def _scan(block: str, allow_semantic: bool):
        # Ø§Ú¯Ø± Ø¨Ù„Ø§Ú© Ø®Ø§Ù… Ø¨ÙˆØ¯ØŒ Ø¨Ø±Ø§ÛŒ exact ÛŒÚ© Ù†Ø³Ø®Ù‡ Ù†Ø±Ù…Ø§Ù„ Ù‡Ù… Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒÙ…
        is_norm = ("\n" not in block)
        norm_block = block if is_norm else _norm(block)

        # --- 1) exact/alias Ø±ÙˆÛŒ Ù…ØªÙ† Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡
        for alias, canon in alias2canon.items():
            if " " in alias:
                if alias in norm_block:
                    found.add(canon)
            else:
                if re.search(rf"(?<!\w){re.escape(alias)}(?!\w)", norm_block):
                    found.add(canon)

        # --- 2) semantic ÙÙ‚Ø· Ø§Ú¯Ø± ÙØ¹Ø§Ù„ Ø¨Ø§Ø´Ø¯ Ùˆ Ø¨Ù„Ø§Ú© Ø®Ø§Ù… Ø¨Ø§Ø´Ø¯
        if allow_semantic and not is_norm and st_model is not None:
            lines = [ln.strip() for ln in block.split("\n") if ln.strip()]
            for canon in canonical_list:
                if canon in found:
                    continue
                ok_line = False
                aliases = skill_map.get(canon, set())
                for ln in lines:
                    # Ø§Ú¯Ø± ÛŒÚ©ÛŒ Ø§Ø² aliasÙ‡Ø§ Ø¯Ø± Ø®Ø· Ø¨ÙˆØ¯ Ø¢Ø³ØªØ§Ù†Ù‡ Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ±
                    if any(a in _norm(ln) for a in aliases):
                        ok, _ = semantic_similarity(canon, ln, threshold=0.67)
                    else:
                        ok, _ = semantic_similarity(canon, ln, threshold=0.72)
                    if ok:
                        ok_line = True
                        break
                if ok_line:
                    found.add(canon)

    # --- ØªØ±ØªÛŒØ¨ Ø§Ø³Ú©Ù† ---
    # exact/alias Ø±ÙˆÛŒ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„
    _scan(skills_txt_norm, allow_semantic=False)
    _scan(exp_txt_norm,    allow_semantic=False)
    _scan(edu_txt_norm,    allow_semantic=False)

    # semantic Ø±ÙˆÛŒ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù… (Ø§Ú¯Ø± use_semantic ÙØ¹Ø§Ù„ Ø§Ø³Øª)
    if use_semantic and st_model is not None:
        _scan(skills_txt_raw, allow_semantic=True)

    matched = [c for c in canonical_list if c in found]
    return matched if matched else ['N/A']

def parse_date_str(date_str: str):
    try:
        return dateparser.parse(
            date_str,
            settings={
                "PREFER_DATES_FROM": "past",  # ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ÛŒ Ø´ØºÙ„ÛŒ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ú¯Ø°Ø´ØªÙ‡â€ŒØ§Ù†Ø¯
                "DATE_ORDER": "DMY"           # Ø±ÙØ¹ Ø§Ø¨Ù‡Ø§Ù… 03/04/2021 â†’ 3 Ø¢ÙˆØ±ÛŒÙ„ 2021
            }
        )
    except Exception:
        return None

def parse_gpa_block(text: str):
    txt = normalize_text(text).lower()
    txt = txt.replace("â„", "/").replace("ï¼", "/")

    gpa_candidates = []

    # 1) Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ Ù…Ø«Ù„ 3.7/4.0 ÛŒØ§ 82/100
    for m in re.finditer(r'([0-9]+(?:\.[0-9]{1,2})?)\s*/\s*([0-9]+(?:\.[0-9]{1,2})?)', txt):
        try:
            raw = float(m.group(1)); scale = float(m.group(2))
            if 0.0 < raw <= scale <= 100:
                gpa_candidates.append(("ratio", round((raw/scale)*4.0, 2)))
        except:
            pass

    # 2) Ø¯Ø±ØµØ¯Ù Ø¨Ø±Ú†Ø³Ø¨â€ŒØ¯Ø§Ø± Ù…Ø«Ù„ GPA: 85%
    for m in re.finditer(r'(?:gpa|cgpa|grade|score)\s*[:\-]?\s*([0-9]{1,3}(?:\.[0-9]{1,2})?)\s*%', txt):
        try:
            val = float(m.group(1))
            if 0 <= val <= 100:
                gpa_candidates.append(("percent_labeled", round((val/100.0)*4.0, 2)))
        except:
            pass

    # 3) Ù…Ù‚Ø¯Ø§Ø± ØªÚ©ÛŒÙ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ¯Ø§Ø± Ù…Ø«Ù„ GPA: 3.7
    for m in re.finditer(r'(?:gpa|cgpa|grade|score)\s*[:\-]?\s*([0-9]{1,2}(?:\.[0-9]{1,2})?)\b', txt):
        try:
            val = float(m.group(1))
            if 0 <= val <= 4.0:
                gpa_candidates.append(("single_labeled", round(val, 2)))
        except:
            pass

    if not gpa_candidates:
        return 'N/A'

    # âœ… Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ†: Ø§ÙˆÙ„ÙˆÛŒØª Â«Ù†Ø³Ø¨ØªÂ» > Â«Ø¯Ø±ØµØ¯Ù Ø¨Ø±Ú†Ø³Ø¨â€ŒØ¯Ø§Ø±Â» > Â«Ù…Ù‚Ø¯Ø§Ø±Ù Ø¨Ø±Ú†Ø³Ø¨â€ŒØ¯Ø§Ø±Â»ØŒ
    # Ùˆ Ø¯Ø§Ø®Ù„ Ù‡Ø± Ø¯Ø³ØªÙ‡ Â«Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø±Â»
    priority = {"ratio": 3, "percent_labeled": 2, "single_labeled": 1}
    gpa_candidates.sort(key=lambda t: (priority[t[0]], t[1]), reverse=True)
    return gpa_candidates[0][1]



def merge_intervals(intervals: list) -> list:
    if not intervals:
        return []
    sorted_ints = sorted(intervals, key=lambda x: x[0])
    merged = [sorted_ints[0]]
    for start, end in sorted_ints[1:]:
        prev_start, prev_end = merged[-1]
        if start <= prev_end:
            merged[-1] = (prev_start, max(prev_end, end))
        else:
            merged.append((start, end))
    return merged

def extract_experience_years(raw: str) -> float:
    secs = split_into_sections(raw)
    exp_text = secs.get('experience', '')
    patterns = [
    r'([A-Za-z]{3,9}\s*\d{4})\s*(?:-|â€“|â€”|to)\s*(Present|Now|Current|[A-Za-z]{3,9}\s*\d{4})',
    r'(\d{1,2}/\d{4})\s*(?:-|â€“|â€”|to)\s*(Present|Now|Current|\d{1,2}/\d{4})',
    r'(\d{4})\s*(?:-|â€“|â€”|to)\s*(Present|Now|Current|\d{4})'
    ]

    now = datetime.now()
    intervals = []
    for pat in patterns:
        for s, e in re.findall(pat, exp_text, flags=re.IGNORECASE):
            ds = parse_date_str(s)
            de = now if re.search(r'present|now|current', e, flags=re.IGNORECASE) else parse_date_str(e)
            if ds and de and de >= ds:
                intervals.append((ds, de))
    merged = merge_intervals(intervals)
    total_months = sum((end.year - start.year) * 12 + (end.month - start.month) for start, end in merged)
    return round(total_months / 12, 2)

def extract_education(raw: str) -> dict:
    """
    ÙÙ‚Ø· GPA Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ù…Ø¯Ø±Ú© Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    Ø®Ø±ÙˆØ¬ÛŒ: {'degree': <label>, 'level': <1..5>, 'gpa': <4.0-scale or 'N/A'>}
    """
    secs = split_into_sections(raw)
    edu_text = secs.get('education', '') or raw

    lines = [ln for ln in edu_text.split('\n') if ln.strip()]
    blocks = []  # [(start, end, degree_label, level)]

    # Ø¨Ù„Ø§Ú©â€ŒÙ‡Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ø±Ú© Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù† (Ù¾Ù†Ø¬Ø±Ù‡â€ŒØ§ÛŒ Ø§Ø² Ú†Ù†Ø¯ Ø®Ø· Ø§Ø·Ø±Ø§ÙØ´)
    for idx, line in enumerate(lines):
        low_line = line.lower()
        for label, syns, lvl in DEGREES:
            keys = DEG_KEYS[label]
            if any(re.search(rf"\b{re.escape(k)}\b", low_line) for k in keys):
                start = max(0, idx - 2)
                end   = min(len(lines), idx + 4)
                blocks.append((start, end, label, lvl))
                break

    if not blocks:
        # Ù‡ÛŒÚ† Ù…Ø¯Ø±Ú©ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯
        return {'degree': 'N/A', 'level': 0, 'gpa': 'N/A'}

    # Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ù…Ø¯Ø±Ú©
    blocks.sort(key=lambda b: b[3], reverse=True)
    top_start, top_end, top_label, top_level = blocks[0]

    # ÙÙ‚Ø· Ø¯Ø± Ø¨Ù„Ø§Ú© Ù‡Ù…Ø§Ù† Ù…Ø¯Ø±Ú© Ø¨Ù‡â€ŒØ¯Ù†Ø¨Ø§Ù„ GPA Ø¨Ú¯Ø±Ø¯
    window_text = '\n'.join(lines[top_start:top_end])
    gpa4 = parse_gpa_block(window_text)  # 'N/A' ÛŒØ§ Ø¹Ø¯Ø¯ Ø±ÙˆÛŒ Ù…Ù‚ÛŒØ§Ø³ 4.0

    return {'degree': top_label, 'level': top_level, 'gpa': gpa4}


# --- Strict major matching: Education-only, phrase-level ---
def _norm_phrase(s: str) -> str:
    s = normalize_text(s).lower()
    s = s.replace("&", " and ")
    s = s.replace("/", " ")
    s = s.replace("-", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _contains_phrase(text_norm: str, phrase_norm: str) -> bool:
    """Ø§Ú¯Ø± phrase Ú†Ù†Ø¯Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ Ø¨ÙˆØ¯: Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø§Ø¯Ù‡Ø›
       Ø§Ú¯Ø± ØªÚ©â€ŒÙˆØ§Ú˜Ù‡ Ø¨ÙˆØ¯: Ø¨Ø§ Ù…Ø±Ø²Ø¨Ù†Ø¯ÛŒ ÙˆØ§Ú˜Ù‡ (word boundary)."""
    if " " in phrase_norm:
        return phrase_norm in text_norm
    return re.search(rf"(?<!\w){re.escape(phrase_norm)}(?!\w)", text_norm) is not None

def extract_major_pct(raw: str, keywords: list) -> float:
    """
    ÙÙ‚Ø· ÙˆÙ‚ØªÛŒ ÛŒÚ©ÛŒ Ø§Ø² Ø¹Ø¨Ø§Ø±Ø§Øª Ø±Ø´ØªÙ‡Ù” Ø¢Ú¯Ù‡ÛŒ Ø¯Ø± Ø¨Ø®Ø´ Education Ø±Ø²ÙˆÙ…Ù‡ Ø¯ÛŒØ¯Ù‡ Ø´ÙˆØ¯ØŒ 100%.
    Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª 0%.
    """
    if not keywords:
        return 0.0

    # ÙÙ‚Ø· Ø¨Ø®Ø´ Education Ø±Ø²ÙˆÙ…Ù‡
    secs = split_into_sections(raw)
    edu_text = secs.get('education', '') or ''
    if not edu_text.strip():
        return 0.0

    txt = _norm_phrase(edu_text)

    for kw in keywords:
        if not kw:
            continue
        kw_norm = _norm_phrase(kw)
        if not kw_norm:
            continue
        if _contains_phrase(txt, kw_norm):
            return 100.0

    return 0.0

def score_candidate(raw: str, req: dict, weights: dict, use_semantic: bool=True) -> dict:
    found = extract_skills(
        raw,
        req.get('orig_skills', []),
        use_semantic=use_semantic,
        skill_map=req.get('_skill_synonyms'),
        alias2canon=req.get('_skill_alias2canon'),
        canonical_list=req.get('_skill_canon_list')
    )

    canon_orig = req.get('orig_skills', [])
    if found and found != ['N/A']:
        skill_pct = round(len(set(found)) / max(1, len(canon_orig)) * 100, 2)
    else:
        skill_pct = 0.0

    # â¬‡ Ø§ÛŒÙ†Ø§ Ø¨Ø§ÛŒØ¯ Ù‡Ù…ÛŒØ´Ù‡ Ø­Ø³Ø§Ø¨ Ø¨Ø´Ù† (Ú†Ù‡ found Ø¨Ø§Ø´Ù‡ Ú†Ù‡ Ù†Ù‡)
    yrs = extract_experience_years(raw)
    exp_req = req.get('min_experience', 0)
    exp_pct = round(min(yrs, exp_req) / exp_req * 100, 2) if exp_req > 0 else 0.0

    edu = extract_education(raw)
    deg_pct = round(edu['level'] / 5 * 100, 2)

    gpa_req = req.get('min_gpa')
    gpa_pct = 0.0
    if gpa_req and edu['gpa'] != 'N/A':
       gpa_pct = round(min(float(edu['gpa']), gpa_req) / gpa_req * 100, 2)

    edu_pct = round((deg_pct + gpa_pct) / 2, 2)
    
    
    major_pct = extract_major_pct(raw, req.get('required_major_keywords', []))

    overall_pct = round(
        skill_pct * weights['skills'] +
        exp_pct   * weights['experience'] +
        edu_pct   * weights['education'] +
        major_pct * weights['major'],
        2
    )

    return {
        'matched_skills': ', '.join(found) if found and found != ['N/A'] else 'N/A',
        'skill_pct': skill_pct,
        'experience_years': yrs,
        'experience_pct': exp_pct,
        'degree': edu['degree'],
        'gpa': edu['gpa'],
        'education_pct': edu_pct,
        'major_pct': major_pct,
        'overall_pct': overall_pct
    }

def process_resume(resume_file, job_req, weights, use_semantic: bool=True):
    raw_text = extract_text_from_pdf(resume_file)
    if not raw_text or len(raw_text.strip()) < 50:
        st.warning(f"Could not extract sufficient text from {resume_file.name}")
        return None
    email = extract_email(raw_text)
    phone = extract_phone(raw_text)
    first_name, last_name = extract_name(raw_text)
    scores = score_candidate(raw_text, job_req, weights, use_semantic=use_semantic)
    return {
        'filename': resume_file.name,
        'position': job_req['position_title'],
        'first_name': first_name,
        'last_name': last_name,
        'email': email,
        'phone': phone,
        **scores,
    }

# ---------- UI ----------

def main():
    st.set_page_config(page_title="Resume Matcher", layout="wide")

    st.markdown("""
        <style>
        .main-title { font-size: 1.13rem !important; font-weight: 700; margin-bottom: 3px; letter-spacing: -0.5px; text-align: left; }
        .sub-title { font-size: 0.98rem !important; font-weight: 400; margin-bottom: 0px; color: #444; text-align: left; }
        html, body, .stApp { font-size: 14px !important; }
        .stButton>button, .stDownloadButton>button, .stSelectbox>div, .stSlider { font-size: 13px !important; }
        .stDataFrame table, .stDataFrame thead, .stDataFrame tbody, .stDataFrame tr, .stDataFrame td, .stDataFrame th { font-size: 12px !important; }
        </style>
    """, unsafe_allow_html=True)

    # Header
    col1, col2, col3 = st.columns([2, 3, 5])
    with col1:
        try:
            st.image("images.png", width=52)
        except Exception:
            pass
        st.markdown("<div class='main-title'>Resume Matcher</div>", unsafe_allow_html=True)
        st.markdown("<div class='sub-title'>Multi-Resume Job Matching Analysis</div>", unsafe_allow_html=True)

    st.sidebar.header("ğŸ“ƒ Upload Job Requirements")
    job_file = st.sidebar.file_uploader("ğŸ“¤ Upload Job Posting JSON", type="json")

    st.sidebar.markdown("### ğŸ“Œ Sample Job JSON")
    sample_json = {
        "position_title": "Data Scientist",
        "skills": ["Python", "Machine Learning", "Data Analysis", "Statistics", "SQL"],
        "min_experience": 3,
        "min_gpa": 3.0,
        "required_major_keywords": ["Computer Science", "Statistics", "Data Science", "computer engineering"]
    }
    st.sidebar.download_button(
        label="ğŸ“¥ Download Sample",
        data=json.dumps(sample_json, indent=2),
        file_name="sample_job_posting.json",
        mime="application/json"
    )

    st.sidebar.header("âš–ï¸ Matching Weights")
    skills_weight     = st.sidebar.slider("Skills Weight", 0.0, 1.0, 0.5, 0.01)
    experience_weight = st.sidebar.slider("Experience Weight", 0.0, 1.0, 0.25, 0.01)
    education_weight  = st.sidebar.slider("Education Weight", 0.0, 1.0, 0.15, 0.01)
    major_weight      = st.sidebar.slider("Major Weight", 0.0, 1.0, 0.10, 0.01)

    st.sidebar.header("âš™ï¸ Options")
    use_semantic = st.sidebar.checkbox("Enable semantic skill matching (SBERT)", value=True)
  
    # ğŸ” ÙˆØ¶Ø¹ÛŒØª Ù„ÙˆØ¯ Ø¨ÙˆØ¯Ù† SBERT (Ø¯Ø§Ø®Ù„ main)
    st.sidebar.markdown("### ğŸ” Runtime status")
    if st_model is not None:
        st.sidebar.write("SBERT loaded: **True**")
        try:
            a = st_model.encode("machine learning", convert_to_tensor=True, normalize_embeddings=True)
            b = st_model.encode("deep learning", convert_to_tensor=True, normalize_embeddings=True)
            sim = float(util.pytorch_cos_sim(a, b).item())
            st.sidebar.write(f"SBERT sanity similarity (ML vs DL): **{sim:.2f}**")
        except Exception as e:
            st.sidebar.warning(f"SBERT test failed: {e}")
    else:
        st.sidebar.write("SBERT loaded: **False**")
        st.sidebar.info("Tip: pre-download the model into ./models/all-MiniLM-L6-v2 or ensure internet/torch is available.")

    
    # Normalize weights
    sum_weights = skills_weight + experience_weight + education_weight + major_weight
    if abs(sum_weights - 1.0) > 1e-6:
        skills_weight     /= sum_weights
        experience_weight /= sum_weights
        education_weight  /= sum_weights
        major_weight      /= sum_weights
        st.sidebar.info("Normalized weights to sum = 1.0")
    weights = {
        'skills': skills_weight,
        'experience': experience_weight,
        'education': education_weight,
        'major': major_weight
    }

    st.header("ğŸ“Š Upload Resumes")
    resume_files = st.file_uploader(
        "ğŸ“¤ Upload multiple resumes (PDF)",
        type="pdf",
        accept_multiple_files=True
    )

    analyze_clicked = st.button("Analyze All Resumes")

    if analyze_clicked:
        if not job_file:
            st.warning("Please upload a Job Posting JSON first in the sidebar!")
            return
        if not resume_files:
            st.warning("Please upload at least one resume (PDF).")
            return

        try:
            job_req = json.load(job_file)
            required_fields = ['position_title', 'skills', 'min_experience']
            for field in required_fields:
                if field not in job_req:
                    st.error(f"Invalid job posting JSON: Missing '{field}' field")
                    return

            # Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ØµØ¯
            job_req['orig_skills'] = job_req.get('skills', [])[:]
            # âš ï¸ Ø¨Ø±Ø§ÛŒ major expand Ù†Ú©Ù†: false positive Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
            # job_req['required_major_keywords'] = expand_keywords(job_req.get('required_major_keywords', []))
            # --- Controls for auto-synonyms (debug/tuning) ---
            sim_threshold = st.sidebar.slider("Synonym sim threshold", 0.5, 0.9, 0.75, 0.01)
            topk = st.sidebar.slider("Synonym top-k per skill", 5, 30, 15, 1)

            # --- Ø³Ø§Ø®Øª Ù‡Ù…â€ŒÙ…Ø¹Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ© Ø§Ø² Ø±ÙˆÛŒ Ù…ØªÙ† Ø±Ø²ÙˆÙ…Ù‡â€ŒÙ‡Ø§ ---
            st.info("Building dynamic skill synonyms from resumesâ€¦")
            candidates = collect_corpus_candidates(resume_files, max_docs=120, max_vocab=5000)
            skill_map, canon_list, alias2canon = build_skill_synonyms_auto(
            job_req['orig_skills'], candidates, st_model, topk=topk, sim_threshold=sim_threshold
            )

            job_req['_skill_synonyms']    = skill_map
            job_req['_skill_canon_list']  = canon_list
            job_req['_skill_alias2canon'] = alias2canon

            # ---- Process resumes (Ø³Ø§Ø®Øª results Ùˆ df) ----
            results = []
            progress_bar = st.progress(0)
            status_text  = st.empty()

            for i, resume_file in enumerate(resume_files):
                # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ø¬Ø¯Ø¯ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ÛŒ ÙØ§ÛŒÙ„
                try:
                    resume_file.seek(0)
                except Exception:
                    pass
                status_text.text(f"Processing {i+1}/{len(resume_files)}: {resume_file.name}")
                result = process_resume(resume_file, job_req, weights, use_semantic=use_semantic)
                progress_bar.progress((i + 1) / max(1, len(resume_files)))
                if result:
                    results.append(result)

            if not results:
                st.error("No valid resumes could be processed.")
                return

            df = pd.DataFrame(results).fillna('N/A')
            df = df.sort_values('overall_pct', ascending=False)

            # Robust candidate label for charts
            def _label(row):
                name = (row['first_name'] if row['first_name']!='N/A' else '') + ' ' + (row['last_name'] if row['last_name']!='N/A' else '')
                name = name.strip()
                if not name:
                    name = row.get('email') or row.get('filename') or 'Candidate'
                return name
            df['candidate_id'] = df.apply(_label, axis=1)

            st.success("Resumes analyzed!")

            st.subheader("ğŸ“Š Summary Results")
            st.dataframe(df[[
                'filename', 'position', 'first_name', 'last_name',
                'skill_pct', 'experience_pct', 'education_pct', 'major_pct', 'overall_pct'
            ]].rename(columns={
                'filename': 'Resume',
                'position': 'Position',
                'first_name': 'First Name',
                'last_name': 'Last Name',
                'skill_pct': 'Skills %',
                'experience_pct': 'Experience %',
                'education_pct': 'Education %',
                'major_pct': 'Major %',
                'overall_pct': 'Match Score'
            }), height=400)

            st.subheader("ğŸ“Š Comparison Chart")
            fig = go.Figure(data=[
                go.Bar(name='Skills Match',     x=df['candidate_id'], y=df['skill_pct']),
                go.Bar(name='Experience Match', x=df['candidate_id'], y=df['experience_pct']),
                go.Bar(name='Overall Match',    x=df['candidate_id'], y=df['overall_pct']),
            ])
            fig.update_layout(
                barmode='group',
                xaxis_title="Candidates",
                yaxis_title="Score (%)",
                legend_title="Metrics",
                height=450,
                template="simple_white"
            )
            st.plotly_chart(fig, use_container_width=True)

            num_top = min(3, len(df))
            st.subheader(f"ğŸ† Top {num_top} Candidates")
            for i in range(num_top):
                candidate = df.iloc[i]
                with st.expander(f"{i+1}. {candidate['first_name']} {candidate['last_name']} - Score: {candidate['overall_pct']}%"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown("**Candidate Information**")
                        st.write(f"**Name:** {candidate['first_name']} {candidate['last_name']}")
                        st.write(f"**Email:** {candidate['email']}")
                        st.write(f"**Phone:** {candidate['phone']}")
                        st.write(f"**Degree:** {candidate['degree']}")
                        st.write(f"**GPA:** {candidate['gpa']}")
                        st.write(f"**Position:** {candidate['position']}")
                    with col2:
                        st.markdown("**Match Scores**")
                        st.metric("Overall Match", f"{candidate['overall_pct']}%")
                        st.metric("Skills Match", f"{candidate['skill_pct']}%")
                        st.metric("Experience Match", f"{candidate['experience_pct']}%")
                        st.metric("Education Match", f"{candidate['education_pct']}%")
                        st.metric("Major Relevance", f"{candidate['major_pct']}%")
                    st.markdown("**Matched Skills**")
                    st.write(candidate['matched_skills'])

            st.subheader("ğŸ’¾ Download Results")
            csv = df.drop(columns=['candidate_id']).to_csv(index=False).encode('utf-8-sig')
            st.download_button(
                label="Download as CSV",
                data=csv,
                file_name='resume_matching_results.csv',
                mime='text/csv'
            )

        except json.JSONDecodeError:
            st.error("Invalid JSON file. Please upload a valid job posting JSON file.")
        except Exception as e:
            st.error(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    main()

